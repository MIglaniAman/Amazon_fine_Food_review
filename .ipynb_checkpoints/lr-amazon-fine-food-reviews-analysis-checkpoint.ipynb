{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression on  Amazon Fine food Review \n",
    "The Amazon fine food dataset consists of reviews of fine foods from amazon\n",
    "All data in one sqlite database. 568,454 food reviews Amazon users left up to October 2012\n",
    "Data includes:\n",
    "\n",
    "* Number of reviews :  568,454 reviews\n",
    "* Number of user : 256,059 users\n",
    "* Total Number of Producst : 74,258 products\n",
    "* Time Span of taking Reviews : from Oct 1999 - Oct 2012\n",
    "* Number of Columns/fields : 10\n",
    "\n",
    "***Attributes Information :***\n",
    "1. Id : Row Id\n",
    "1. ProductId :Unique identifier for the product (74258 unique values)\n",
    "1. UserId :Unqiue identifier for the user (256059 unique values)\n",
    "1. ProfileName : Profile name of the user (218418 unique values )\n",
    "1. HelpfulnessNumerator : Number of users who found the review helpful \n",
    "1. HelpfulnessDenominator : Number of users who indicated whether they found the review helpful or not \n",
    "1. Score : Rating between 1 and 5\n",
    "1. Time : Timestamp for the review\n",
    "1. Summary : Brief summary of the review (295744 unique values)\n",
    "1. Text : Text of the review (393579 unique values)\n",
    "\n",
    "> **Objective :** *Given a review ,determine whether a review is postive or negative*\n",
    "\n",
    "**Q].** *How to determine if a review is postive or negative?*\n",
    "\n",
    "**Ans.** We can use the Score/rating. A rating of 4 or 5 could be considered as positive review & A rating of 1 or 2 can be consider as negative review . A rating of 3 can be consider as neutral and can be ignored . This is the approximation and proxy way to approach way of determing the polarity(positivity/negativity) of reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loadig the data \n",
    "The dataset is available in two forms\n",
    "1. .csv form\n",
    "2. Sqlite Database\n",
    "\n",
    "In order to load the database ,I have use the Sqlite Database as it easier to query the data and visualise the data sufficienlty . Here as we only want to get the global sentiment of the recommnedation(Positive/Negative), We will purposefully ignore all the scores equal to 3 ,If the Score is above 3 then the reviews will be \"Positive\",Otherwise it will \"Negative\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing important library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#mterics \n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import os\n",
    "import pickle \n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readind Data from Sqlite database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "unable to open database file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-2b788a3928b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Using the sqlite table to read data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msqlite3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/kaggle/input/amazon-fine-food-reviews/database.sqlite'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# Filtering only Positive and Negative Reviews i.e not taking into consideration those reviews with score=3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mfiltered_data\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_sql_query\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\"SELECT * FROM Reviews WHERE Score!=3\"\"\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#Given reviews with score>3 a postive rating \"1\" and reviews with score <3 a negative \"0\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOperationalError\u001b[0m: unable to open database file"
     ]
    }
   ],
   "source": [
    "#Using the sqlite table to read data\n",
    "con = sqlite3.connect('/kaggle/input/amazon-fine-food-reviews/database.sqlite')\n",
    "# Filtering only Positive and Negative Reviews i.e not taking into consideration those reviews with score=3\n",
    "filtered_data= pd.read_sql_query(\"\"\"SELECT * FROM Reviews WHERE Score!=3\"\"\",con)\n",
    "#Given reviews with score>3 a postive rating \"1\" and reviews with score <3 a negative \"0\"\n",
    "#constucting a function\n",
    "def partition(x):\n",
    "    if x<3:\n",
    "        return 0\n",
    "    return 1\n",
    "#Changing reviews with score<3 to be negative \"0\" and score > 3 to be positive \"1\"\n",
    "actualscore=filtered_data['Score']\n",
    "Positve_Negative=actualscore.map(partition)\n",
    "filtered_data['Score']=Positve_Negative\n",
    "print(\"No. of data points in Dataset:\",filtered_data.shape)\n",
    "filtered_data.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "### Data Deduplication{The most important part of cleaning data}\n",
    "It was observes that(as shown in table below) that the reviews data had many deduplicates entries .Hence it is neccessary to remove the deduplication in order to get unbaised results for the Analysis of the data.\n",
    "Following is the example given below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display= pd.read_sql_query(\"\"\"SELECT * FROM Reviews WHERE Score !=3 AND UserId ='AR5J8UI46CURR' ORDER BY ProductId \"\"\",con)\n",
    "display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As it can be seen above the same user has multiple reviews of the same values for HelpfulnessNumerator, HelpfulnessDenominator, Score, Time, Summary and Text and on doing analysis it was found that ProductId=B000HDOPZG was Loacker Quadratini Vanilla Wafer Cookies, 8.82-Ounce Packages (Pack of 8)**\n",
    "\n",
    "**ProductId=B000HDL1RQ was Loacker Quadratini Lemon Wafer Cookies, 8.82-Ounce Packages (Pack of 8) and so on**\n",
    "\n",
    "**It was inferred after analysis that reviews with same parameters other than ProductId belonged to the same product just having different flavour or quantity. Hence in order to reduce redundancy it was decided to eliminate the rows having same parameters.**\n",
    "\n",
    "**The method used for the same was that we first sort the data according to ProductId and then just keep the first similar product review and delelte the others. for eg. in the above just the review for ProductId=B000HDL1RQ remains. This method ensures that there is only one representative for each product and deduplication without sorting would lead to possibility of different representatives still existing for the same product**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting data according to ProductId in ascending order\n",
    "sorted_data=filtered_data.sort_values(\"ProductId\",axis=0,ascending=True,inplace=False,kind='quicksort',na_position='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deduplication entries\n",
    "final_data=sorted_data.drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"},keep='first',inplace=False)\n",
    "final_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking to see how much % of data still remains\n",
    "print('The total data remain after cleaning data ',(final_data['Id'].size*1.0)/(filtered_data['Id'].size*1.0)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data[final_data['HelpfulnessNumerator']>final_data['HelpfulnessDenominator']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display=pd.read_sql_query(\"\"\"SELECT * FROM Reviews WHERE Score!=3 AND Id=44737 OR Id=64422\n",
    "ORDER BY ProductID\"\"\",con)\n",
    "display.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** It was seen that in 2 rows given above the value of HelpfullnessNumerator is Greater than HelpfullnessDenominator which is not practically hence such rows are too removed from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data=final_data[final_data['HelpfulnessNumerator']<=final_data['HelpfulnessDenominator']]\n",
    "final_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before starting the next phase of text preprocessing lets see the no. of entries left\n",
    "print(final_data.shape)\n",
    "#How many positive and negative reviews are present in our dataset?\n",
    "final_data['Score'].value_counts()\n",
    "sns.countplot(final_data['Score'])\n",
    "plt.show()\n",
    "sns.countplot(data=final_data,y='Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "**Now we have finished deduplication. Now our data requires some preprocessing before we go on futher anlysis and make the prediction model Hence in the preprocessing phase we do the following steps given below**\n",
    "\n",
    "1. Begin by removing html tags\n",
    "1. Removing any punctution or limited set of special character:like ,or . or # etc\n",
    "1. Check the words is made up of english letters and is not alpha-numeric\n",
    "1. Check to see if the length of the words is greater than 2 (as it was research that there is no adjective in 2 letter)\n",
    "1. Convert the words to lowercase\n",
    "1. Remove stopwords\n",
    "1. Snowball stemming the word(it is observed that Snoball stemming is better that Porter stemming)\n",
    "\n",
    "***After this we will collect the words and will use to describe positive and negative reviews.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding HTML tags in sentences\n",
    "import re\n",
    "i=0\n",
    "for review in final_data['Text'].values:\n",
    "    if (len(re.findall('<.*?>',review))):\n",
    "        print(review)\n",
    "        break\n",
    "    i=i+1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "stop = set(stopwords.words('english')) # creating set of stopwords\n",
    "print(stop) # it will to show all the stopwords in NLTK\n",
    "excluding_stop = ['against','not','don', \"don't\",'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\",\n",
    "             'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", \n",
    "             'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\",'shouldn', \"shouldn't\", 'wasn',\n",
    "             \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "print('****'*15)\n",
    "\n",
    "stop = [word for word in stop if word not in excluding_stop]\n",
    "print(' ')\n",
    "print(stop)\n",
    "snowstem=nltk.stem.SnowballStemmer('english') # intialising the snowball stemmer\n",
    "print(' ')\n",
    "print('****'*15)\n",
    "print(\"base_word of tasty:\", snowstem.stem('tasty')) # it will tell us the base word or do stemming\n",
    "\n",
    "# creating a function to clean the word of any html-tags. The function will remove the html tag and evrything between them with \"1\" space \n",
    "def cleanhtml(sentence):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, ' ', sentence)\n",
    "    return cleantext\n",
    "# creating a function to clean the punctuation or special characters .The function will create the punctuation with empty string\n",
    "def cleanpunc(sentence):\n",
    "    cleaned= re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "    cleaned= re.sub(r'[.|,|)|(|\\|/]',r'',cleaned)\n",
    "    return cleaned\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing some random reviews\n",
    "sent_0 = final_data['Text'].values[6]\n",
    "print(sent_0)\n",
    "print(\"=\"*50)\n",
    "\n",
    "sent_1000 = final_data['Text'].values[1000]\n",
    "print(sent_1000)\n",
    "print(\"=\"*50)\n",
    "\n",
    "sent_1500 = final_data['Text'].values[1500]\n",
    "print(sent_1500)\n",
    "print(\"=\"*50)\n",
    "\n",
    "sent_4900 = final_data['Text'].values[4900]\n",
    "print(sent_4900)\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re    #Tutorial about Python regular expressions: https://pymotw.com/2/re/\n",
    "# remove urls from text python : *https://stackoverflow.com/a/40823105/4084039*\n",
    "sent_0=re.sub(r'https\\S+','',sent_0)\n",
    "sent_1000=re.sub(r'https\\S+','',sent_1000)\n",
    "sent_1500=re.sub(r'https\\S+','',sent_1500)\n",
    "sent_4900=re.sub(r'http\\S+', '', sent_4900)\n",
    "\n",
    "print(sent_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup=BeautifulSoup(sent_0,'lxml')\n",
    "text=soup.get_text()\n",
    "print(text)\n",
    "print(\"=\"*50)\n",
    "\n",
    "soup=BeautifulSoup(sent_1000,'lxml')\n",
    "text=soup.get_text()\n",
    "print(text)\n",
    "print(\"=\"*50)\n",
    "\n",
    "soup=BeautifulSoup(sent_1500,'lxml')\n",
    "text=soup.get_text()\n",
    "print(text)\n",
    "print(\"=\"*50)\n",
    "\n",
    "soup=BeautifulSoup(sent_4900,'lxml')\n",
    "text=soup.get_text()\n",
    "print(text)\n",
    "print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "def decontracted(phrase):\n",
    "    phrase=re.sub(r\"won't\",'will not',phrase)\n",
    "    phrase=re.sub(r\"can\\'t\",'can not',phrase)\n",
    "    phrase=re.sub(r\"n\\'t\",'not',phrase)\n",
    "    phrase=re.sub(r\"\\'re\",'are',phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rremove words with no. python  # https://stackoverflow.com/a/18082370/4084039\n",
    "sent_0=re.sub(\"\\S\\d\\S\",\"\",sent_0).strip()\n",
    "print(sent_0)\n",
    "print(\"=\"*50)\n",
    "\n",
    "#remove specail charachter # https://stackoverflow.com/a/5843547/4084039\n",
    "sent_1500=re.sub('[^A-Za-z0-9]+',' ',sent_1500)\n",
    "print(sent_1500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining all the statement above\n",
    "from tqdm import tqdm\n",
    "preprocessed_review=[]\n",
    "for sentence in tqdm(final_data['Text'].values):\n",
    "    sentence=re.sub(r\"http\\S+\", \"\",sentence)\n",
    "    sentence=BeautifulSoup(sentence,'lxml').get_text()\n",
    "    sentence=decontracted(sentence)\n",
    "    sentence=re.sub(\"\\S*\\d\\S*\",\"\",sentence).strip()\n",
    "    sentence=re.sub('[^A-Za-z]+', ' ', sentence)\n",
    "    # https://gist.github.com/sebleier/554280\n",
    "    sentence=' '.join(e.lower() for e in sentence.split() if e.lower() not in stop)\n",
    "    preprocessed_review.append(sentence.strip())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "preprocessed_summary=[]\n",
    "for sentence in tqdm(final_data['Summary'].values):\n",
    "    sentence=re.sub(r\"http\\S+\", \"\",sentence)\n",
    "    sentence=BeautifulSoup(sentence,'lxml').get_text()\n",
    "    sentence=decontracted(sentence)\n",
    "    sentence=re.sub(\"\\S*\\d\\S*\",\"\",sentence).strip()\n",
    "    sentence=re.sub('[^A-Za-z]+', ' ', sentence)\n",
    "    # https://gist.github.com/sebleier/554280\n",
    "    sentence=' '.join(e.lower() for e in sentence.split() if e.lower() not in stop)\n",
    "    preprocessed_summary.append(sentence.strip())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SENTENCES Containing HTML tags\n",
    "import re\n",
    "i=0;\n",
    "for sentence in final_data['Text'].values:\n",
    "    if (len(re.findall('<.*?>',sentence))):\n",
    "        break;\n",
    "    print(\"=\"*50)\n",
    "    print(i,\")>>\",sentence)\n",
    "    i=i+1\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "print(\"Total no. of sentences containing html tags\",i)\n",
    "#Set of stopwords\n",
    "#stop_words=set(stopwords.word('english'))\n",
    "\n",
    "# we are removing the words from the stop words list: 'no', 'nor', 'not' for semantic meanig in bigrams and trigram\n",
    "stopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn',  'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn',  'weren', \\\n",
    "            'won', \"won't\", 'wouldn'])\n",
    "stemming=nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "#creating funtion to remove html tags from words \n",
    "#https://stackoverflow.com/questions/9662346/python-code-to-remove-html-tags-from-a-string\n",
    "#1st:Using regrex\n",
    "def cleanhtml(sentencs):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', sentencs)\n",
    "    return cleantext\n",
    "#OR\n",
    "#2ndUsing Beautiful\n",
    "from bs4 import BeautifulSoup\n",
    "cleantext=BeautifulSoup(sentence,\"lxml\").text\n",
    "\n",
    "##function to clean the word of any punctuation or special characters\n",
    "def cleanpunc(sentence):\n",
    "    #clean =re.sub('[^A-Za-z0-9]+', '', sentence)\n",
    "    cleanp=re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "    clean_punc=re.sub(r'[.|,|)|(|\\|/]',r' ',cleanp)\n",
    "    return clean_punc\n",
    "print('stop words are :',stopwords)\n",
    "print(\"*********************\")\n",
    "print('base word for tasty is :',stemming.stem('tasty'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "#the code will take time to run \n",
    "if not os.path.isfile('finalsqlite'):\n",
    "    final_string=[]\n",
    "    all_postive_words=[]\n",
    "    all_negative_words=[]\n",
    "    for i , sentence in enumerate(tqdm(final_data['Text'].values)):\n",
    "        filter_sentence=[]\n",
    "        sent=cleanhtml(sentence)\n",
    "        for words in sent.split():\n",
    "            # we have used cleanpunc(w).split(), one more split function here because consider w=\"abc.def\", cleanpunc(w) will return \"abc def\"\n",
    "            # if we dont use .split() function then we will be considring \"abc def\" as a single word, but if you use .split() function we will get \"abc\", \"def\"\n",
    "            for clean_words in cleanpunc(words).split():\n",
    "                if ((clean_words.isalpha())&(len(clean_words)>2)):\n",
    "                    if (clean_words.lower() not in stopwords):\n",
    "                        s = (stemming.stem(clean_words.lower())).encode(encoding='UTF-8')\n",
    "                        filter_sentence.append(s)\n",
    "                        if (final_data['Score'].values)[i]==1:\n",
    "                            #list of all words used to describe positive reviews\n",
    "                            all_postive_words.append(s)  \n",
    "                            \n",
    "                        if (final_data['Score'].values)[i]==0:\n",
    "                            #list of all words used to describe negative reviews\n",
    "                            all_negative_words.append(s)  \n",
    "                    else:\n",
    "                        continue\n",
    "                else:\n",
    "                    continue\n",
    "        str1=b\" \".join(filter_sentence)#final string of cleaned words\n",
    "        #print(\"***********************************************************************\")\n",
    "        final_string.append(str1)\n",
    "        i+=1\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding a column of CleanedText which displays the data after pre-processing of the review\n",
    "final_data['Cleaned Text']=preprocessed_review\n",
    "final_data['Cleaned_Summary']=preprocessed_summary\n",
    "\n",
    "#print(final_data[['Text','Clean_text']])\n",
    "#final_data['Cleaned Text']=final_data['Cleaned Text'].str.decode('UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting time in unit=sec\n",
    "final_data['Time']=pd.to_datetime(final_data['Time'],unit='s')\n",
    "final_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorting dataset based on 'Time' feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data=final_data.sort_values('Time',axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')\n",
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the final table into an SQLITE table for future\n",
    "connection=sqlite3.connect('final12.sqlite')\n",
    "c=connection.cursor\n",
    "connection.text_factory=str\n",
    "final_data.to_sql('Reviews',connection,schema=None,if_exists='replace',index=True,index_label=True,chunksize=None,dtype=None)\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5: Apply Logistic Regression\n",
    "1. Apply Logistic Regression on these feature sets\n",
    "    * Review text, preprocessed one converted into vectors using (BOW)\n",
    "    * Review text, preprocessed one converted into vectors using (TFIDF)\n",
    "    * Review text, preprocessed one converted into vectors using (AVG W2v)\n",
    "    * Review text, preprocessed one converted into vectors using (TFIDF W2v)\n",
    "\n",
    "1. Hyper paramter tuning (find best hyper parameters corresponding the algorithm that you choose)\n",
    "    * Finding the best hyper parameter which will give the maximum AUC value\n",
    "                OR\n",
    "    * Finding the best hyper paramter using k-fold cross validation or simple cross validation data\n",
    "                OR\n",
    "    * Use gridsearch cv or randomsearch cv or you can also write your own for loops to do this task of hyperparameter tuning\n",
    "\n",
    "1. Pertubation Test\n",
    "    * Get the weights W after fit your model with the data X.\n",
    "    * Add a noise to the X (X' = X + e) and get the new data set X' (if X is a sparse matrix, X.data+=e)\n",
    "    * Fit the model again on data X' and get the weights W'\n",
    "    * Add a small eps value(to eliminate the divisible by zero error) to W and W’ i.e W=W+10^-6 and W’ = W’+10^-6\n",
    "    * Now find the % change between W and W' (| (W-W') / (W) |)*100)\n",
    "    * Calculate the 0th, 10th, 20th, 30th, ...100th percentiles, and observe any sudden rise in the values of percentage_change_vector\n",
    "    * Ex: consider your 99th percentile is 1.3 and your 100th percentiles are 34.6, there is sudden rise from 1.3 to 34.6, now calculate the 99.1, 99.2, 99.3,..., 100th percentile values and get the proper value after which there is sudden rise the values, assume it is 2.5\n",
    "    * Printing the feature names whose % change is more than a threshold x(in our example it's 2.5)\n",
    "\n",
    "1. Sparsity\n",
    "    * Calculate sparsity on weight vector obtained after using L1 regularization\n",
    "    \n",
    "1. Feature importance\n",
    "    * Get top 10 important features for both positive and negative classes separately.\n",
    "\n",
    "1. Feature engineering\n",
    "    * To increase the performance of your model, we can also experiment with with feature engineering like :\n",
    "        * Taking length of reviews as another feature.\n",
    "        * Considering some features from review summary as well.\n",
    "\n",
    "1. Representation of results\n",
    "    * ploting the performance of model both on train data and cross validation data for each hyper parameter, like shown in the figure.\n",
    "    * Once after you found the best hyper parameter, you need to train your model with it, and find the AUC on test data and plot the ROC curve on both train and test.\n",
    "    * Along with plotting ROC curve, you need to print the confusion matrix with predicted and original labels of test data points. Please visualize your confusion matrices using seaborn heatmaps.\n",
    "\n",
    "1. Conclusion\n",
    "    * summarize the results at the end of the notebook, summarize it in the table format. To print out a table please refer to this prettytable library link\n",
    "\n",
    "\n",
    "1. There will be an issue of data-leakage if you vectorize the entire data and then split it into train/cv/test.\n",
    "2. To avoid the issue of data-leakag, make sure to split your data first and then vectorize it.\n",
    "3. While vectorizing your data, apply the method fit_transform() on you train data, and apply the method transform() on cv/test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking equal no. of negative and positive data points\n",
    "data_pos=final_data[final_data['Score']==1].sample(n=60000)\n",
    "data_neg=final_data[final_data['Score']==0].sample(n=57000)\n",
    "final=pd.concat([data_pos,data_neg])\n",
    "final.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Logistic Regression on BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y= final['Score']\n",
    "X = final['Cleaned Text']\n",
    "print(\"Shape of X\",X.shape)\n",
    "print(\"Shape of y\",y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_tr ,X_test,y_tr,y_test = train_test_split(X,y,test_size=.30,random_state=0)\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X_tr, y_tr, test_size=.30, random_state=0)\n",
    "print('Shape of X_train is :',X_train.shape)\n",
    "print('Shape of y_train is :',y_train.shape)\n",
    "print(\"****\"*6)\n",
    "print('Shape of X_Cv is :',X_cv.shape)\n",
    "print('Shape of y_cv is :',y_cv.shape)\n",
    "print(\"****\"*6)\n",
    "print('Shape of X_test is :',X_test.shape)\n",
    "print('Shape of y_test is :',y_test.shape)\n",
    "print(\"****\"*6)\n",
    "print('Shape of X_tr is :',X_tr.shape)\n",
    "print('Shape of y_tr is :',y_tr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Apply Logistic Regression with L2 regularization on BOW "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# *Converting text into vectors using BOW\n",
    "bow_count_vect=CountVectorizer()\n",
    "bow_count_vect.fit(X_train)\n",
    "# Using the fited CountVectorizer to convert text to vectors\n",
    "Xtrain_bow=bow_count_vect.transform(X_train)\n",
    "Xcv_bow=bow_count_vect.transform(X_cv)\n",
    "Xtest_bow=bow_count_vect.transform(X_test)\n",
    "print(\"After vectorizations\")\n",
    "print(Xtrain_bow.shape, y_train.shape)\n",
    "print(Xcv_bow.shape, y_cv.shape)\n",
    "print(Xtest_bow.shape, y_test.shape)\n",
    "#print(X_tr_bow.shape, y_tr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOU SHOULD NOT DO LIKE THIS\n",
    "1. THE VOCABULARY SHOULD BUILT ONLY WITH THE WORDS OF TRAIN DATA\n",
    "    * vectorizer = CountVectorizer()\n",
    "    * x_train_bow = vectorizer.fit_transform(X_train)\n",
    "    * x_cv_bow = vectorizer.fit_transform(X_cv)\n",
    "    * x_test_bow = vectorizer.fit_transform(X_test)\n",
    "    \n",
    "2. DATA LEAKAGE PROBLEM: IF WE DO LIKE THIS WE ARE LOOKING AT THE TEST DATA BEFORE MODELING\n",
    "    * vectorizer = CountVectorizer()\n",
    "    * X_bow = vectorizer.fit_transfomr(X)\n",
    "    * X_train, X_test, y_train, y_test = train_test_split(X_bow, Y, test_size=0.33)\n",
    "    \n",
    "3. YOU SHOULD PASS THE PROBABILITY SCORES NOT THE PREDICTED VALUES\n",
    "    * y_pred = neigh.predict(X)\n",
    "    * roc_auc_score(y_ture,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler(with_mean=False)\n",
    "scaler.fit(Xtrain_bow)\n",
    "X_tr_bow=scaler.transform(Xtrain_bow)\n",
    "X_test_bow=scaler.transform(Xtest_bow)\n",
    "X_cv_bow=scaler.transform(Xcv_bow)\n",
    "print(\"After standardising\")\n",
    "print(X_tr_bow.shape, y_train.shape)\n",
    "print(X_cv_bow.shape, y_cv.shape)\n",
    "print(X_test_bow.shape, y_test.shape)\n",
    "#print(X_tr_bow.shape, y_tr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.model_selection import cross_val_score\n",
    "C =[0.00001,0.0001,0.001,0.01,0.1,1,2.1,10,100,1000,10000,100000]#C=1/lambda\n",
    "auc_train=[]\n",
    "auc_cv=[]\n",
    "for c in C:\n",
    "    model_logreg=LogisticRegression(penalty='l2',C=c)\n",
    "    model_logreg.fit(X_tr_bow,y_train)\n",
    "    \n",
    "    y_pred_train=model_logreg.predict_proba(X_tr_bow)[:,1]\n",
    "    auc_train.append(roc_auc_score(y_train,y_pred_train))\n",
    "    \n",
    "    y_pred_cv=model_logreg.predict_proba(X_cv_bow)[:,1]\n",
    "    auc_cv.append(roc_auc_score(y_cv,y_pred_cv))\n",
    "    \n",
    "    auc_score_cv =roc_auc_score(y_cv,y_pred_cv)\n",
    "    print(c,\"-------->\",auc_score_cv )\n",
    "    \n",
    "optimal_C=C[auc_cv.index(max(auc_cv))]\n",
    "c=[math.log(x) for x in C]\n",
    "print('Optimal C  is :',optimal_C)\n",
    "fig =plt.figure()\n",
    "plt.plot(c,auc_train,color='green',label='AUC train')\n",
    "plt.plot(c,auc_cv,color='red',label='AUC test')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('log(alpha')\n",
    "plt.ylabel('AUC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                           Or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding optimal_alpha by using 10-fold_cv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "alpha_C=[0.00001,0.0001,0.001,0.01,0.1,1,2.1,10,100,1000,10000,100000]\n",
    "cv_scores=[]\n",
    "#perfrom 10-fold cv\n",
    "for i in tqdm(alpha_C):\n",
    "    model_logreg=LogisticRegression(penalty='l2',C=i)\n",
    "    scores=cross_val_score(model_logreg,X_tr_bow,y_train,cv=10)\n",
    "    cv_scores.append(scores.mean())\n",
    "#Changing to misclassification error\n",
    "MSE=[1 - x for x in cv_scores]\n",
    "\n",
    "#determing best alpha\n",
    "optimal_C=alpha_C[MSE.index(min(MSE))]\n",
    "aplha_C=[math.log(x) for x in alpha_C]\n",
    "print('\\nThe optimal number of alpha is ',optimal_C)\n",
    "#Plot the misclassfication error vs alpha\n",
    "plt.plot(aplha_C,MSE,marker='*')\n",
    "plt.title(\"Misclassification Error vs alpha\")\n",
    "plt.xlabel('value of alpha')\n",
    "plt.ylabel('Misclassification Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC for labmda=\n",
    "lr=LogisticRegression(penalty='l2',C=optimal_C)\n",
    "lr.fit(X_tr_bow,y_train)\n",
    "predi=lr.predict_proba(X_test_bow)[:,1]\n",
    "fpr1, tpr1, thresholds1 = metrics.roc_curve(y_test, predi)\n",
    "pred=lr.predict_proba(X_tr_bow)[:,1]\n",
    "fpr2,tpr2,thresholds2=metrics.roc_curve(y_train,pred)\n",
    "fig = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "ax.plot(fpr1, tpr1, label='Test ROC ,auc='+str(roc_auc_score(y_test,predi)))\n",
    "ax.plot(fpr2, tpr2, label='Train ROC ,auc='+str(roc_auc_score(y_train,pred)))\n",
    "plt.title('ROC')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix using heat map for test data\n",
    "from sklearn.metrics import  confusion_matrix\n",
    "logreg=LogisticRegression(penalty='l2',C=optimal_C)\n",
    "logreg.fit(X_tr_bow,y_train)\n",
    "pred=logreg.predict(X_test_bow)\n",
    "import seaborn as sns\n",
    "conf_mat=confusion_matrix(y_test,pred)\n",
    "class_label = [\"negative\", \"positive\"]\n",
    "df=pd.DataFrame(conf_mat,columns=class_label,index=class_label)\n",
    "sns.heatmap(df,annot=True,fmt='d')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Performing pertubation test (multicollinearity check) on BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Weights_before=logreg.coef_\n",
    "\n",
    "X_e=X_tr_bow\n",
    "print('Shape of X_e before adding noise ',X_e.shape)\n",
    "X_e.data=X_e.data+np.random.normal(loc=0,scale=0.0001,size=X_e.data.shape)\n",
    "print('Shape of X_e.data',X_e.data.shape)\n",
    "print('Shape of X_e after adding noise',X_e.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Logistic regression With X_e\n",
    "lr_e=LogisticRegression(penalty='l2',C=optimal_C)\n",
    "lr_e.fit(X_e,y_train)\n",
    "W_after=lr_e.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(Weights_before[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to eliminate divisible by zero error we will add 10^-6 to W_before and W_after\n",
    "Weights_before+=10**-6\n",
    "W_after+=10**-6\n",
    "\n",
    "per_vectors=[]\n",
    "\n",
    "print(len(Weights_before[0]))\n",
    "\n",
    "for i in range(len(Weights_before[0])):\n",
    "    val=W_after[0][i]-Weights_before[0][i]\n",
    "    val/=Weights_before[0][i]\n",
    "    per_vectors.append(val)\n",
    "    \n",
    "Original_per_vect=np.absolute(per_vectors)\n",
    "per_vectors=sorted(np.absolute(per_vectors))[::-1]\n",
    "#percentage change iin vectors \n",
    "per_vectors[:10]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(per_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating percentile from 0 to 100:\n",
    "for i in range(11): #https://www.geeksforgeeks.org/numpy-percentile-in-python/ \n",
    "    print(str(i*10)+'th percentile = '+str(np.percentile(per_vectors,i*10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there is sudden rise in percentile from 90 to 100\n",
    "#calculating percentile from 9 to 100:\n",
    "\n",
    "for i in range(90,101): #https://www.geeksforgeeks.org/numpy-percentile-in-python/ \n",
    "    print(str(i)+'th percentile = '+str(np.percentile(per_vectors,i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there is sudden rise in percentile from 99 to 100\n",
    "#calculating percentile from 99 to 100:\n",
    "\n",
    "for i in range(1,11): #https://www.geeksforgeeks.org/numpy-percentile-in-python/ \n",
    "    print(str(99+(10**-1)*i)+'th percentile = '+str(np.percentile(per_vectors,99+(10**-1)*i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding features from 99.9th percentile to 100th percentile\n",
    "print('features from 99.9th percentile to 100th percentile')\n",
    "original_per_vect=Original_per_vect.tolist()\n",
    "all_features=bow_count_vect.get_feature_names()\n",
    "#for i in range(1,11): #https://www.geeksforgeeks.org/numpy-percentile-in-python/ \n",
    "    #index=original_per_vect.index(np.percentile(per_vectors,99.9+(10**-2)*i))\n",
    "    #print(all_features[index])\n",
    "#indx=original_per_vect.index(index)\n",
    "#print(all_features[indx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance on BOW,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight=logreg.coef_\n",
    "#x = np.array([3, 1, 2])\n",
    "#np.argsort(x)\n",
    "#array([1, 2, 0])\n",
    "pos_index=np.argsort(weight)[:,::-1]\n",
    "neg_index=np.argsort(weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Top 10 important features of positive class from\n",
    "print('Top 10 important features of positive class ')\n",
    "for i in list(pos_index[0][:10]):\n",
    "    print(all_features[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Top 10 important features of positive class from\n",
    "print('Top 10 important features of neagtive class ')\n",
    "for i in list(neg_index[0][:10]):\n",
    "    print(all_features[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Apply Logistic Regression with L2 regularization on Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Conveting text into vectors using Tf_idf\n",
    "\n",
    "tf_idf_vect=TfidfVectorizer(ngram_range=(1,2))  #taking both unigram and bigram\n",
    "tf_idf_vect.fit(X_train)\n",
    "# Using the transrom fucntion TfidfVectorizer  convert text to vectors\n",
    "Xtrain_tf_idf=tf_idf_vect.transform(X_train)\n",
    "Xcv_tf_idf   =tf_idf_vect.transform(X_cv)\n",
    "Xtest_tf_idf =tf_idf_vect.transform(X_test)\n",
    "print(\"After vectorizations\")\n",
    "print(Xtrain_tf_idf.shape, y_train.shape)\n",
    "print(Xcv_tf_idf.shape, y_cv.shape)\n",
    "print(Xtest_tf_idf.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.model_selection import  cross_val_score\n",
    "C =[0.00001,0.0001,0.001,0.01,0.1,1,2.1,10,100,1000,10000,100000]#C=1/lambda\n",
    "auc_train=[]\n",
    "auc_cv=[]\n",
    "for c in C:\n",
    "    model_logreg=LogisticRegression(penalty='l2',C=c)\n",
    "    model_logreg.fit(Xtrain_tf_idf,y_train)\n",
    "    \n",
    "    y_pred_train=model_logreg.predict_proba(Xtrain_tf_idf)[:,1]  #[:,1] its used for saving ony positve class\n",
    "    auc_train.append(roc_auc_score(y_train,y_pred_train))\n",
    "    \n",
    "    y_pred_cv=model_logreg.predict_proba(Xcv_tf_idf)[:,1]\n",
    "    auc_cv.append(roc_auc_score(y_cv,y_pred_cv))\n",
    "    \n",
    "    auc_score_cv=roc_auc_score(y_cv,y_pred_cv)\n",
    "    print(c,\"-------->\",auc_score_cv )\n",
    "\n",
    "optimal_c=C[auc_cv.index(max(auc_cv))]\n",
    "c=[math.log(x) for x in C]\n",
    "print(\"Optimal c:\",optimal_c)\n",
    "fig=plt.figure()\n",
    "plt.plot(c,auc_train,color='green',label='Auc_Train')\n",
    "plt.plot(c,auc_cv,color='red',label='Auc_Test')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('log(alpha')\n",
    "plt.ylabel('AUC')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Another way of finding optimal_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding optimal_alpha by using 10-fold_cv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "alpha_C=[0.00001,0.0001,0.001,0.01,0.1,1,2.1,10,100,1000,10000,100000]\n",
    "cv_score_tfidf=[]\n",
    "#perform 10-foldcv\n",
    "for i in tqdm(alpha_C):\n",
    "    model_logreg_tfidf=LogisticRegression(penalty='l2',C=i)\n",
    "    score_tfidf=cross_val_score(model_logreg_tfidf,Xtrain_tf_idf,y_train,cv=10)\n",
    "    cv_score_tfidf.append(score_tfidf.mean())\n",
    "    \n",
    "#Changing to misclassification error\n",
    "MSE=[1-x for x in cv_score_tfidf]\n",
    "\n",
    "#determine  best alpha\n",
    "optimal_C_tfidf=alpha_C[MSE.index(min(MSE))]\n",
    "print(\"Optimal c:\",optimal_C_tfidf)\n",
    "alpha_C=[math.log(x) for x in alpha_C]\n",
    "#Plot the misclassfication error vs alpha\n",
    "plt.plot(alpha_C,MSE,marker='*')\n",
    "plt.title(\"Misclassification Error vs alpha\")\n",
    "plt.xlabel('value of alpha')\n",
    "plt.ylabel('Misclassification Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roc for allpha =1000\n",
    "model=LogisticRegression(penalty='l2',C=optimal_C_tfidf)\n",
    "model.fit(Xtrain_tf_idf,y_train)\n",
    "\n",
    "y_test_pred=model.predict_proba(Xtest_tf_idf)[:,1]\n",
    "fpr1,tpr1,thresholds1=metrics.roc_curve(y_test,y_test_pred)\n",
    "\n",
    "y_train_pred=model.predict_proba(Xtrain_tf_idf)[:,1]\n",
    "fpr2,tpr2,thresholds2=metrics.roc_curve(y_train,y_train_pred)\n",
    "\n",
    "fig=plt.figure()\n",
    "ax=plt.subplot(111)\n",
    "ax.plot(fpr1,tpr1,label=\"Test Roc,auc=\"+str(roc_auc_score(y_test,y_test_pred)))\n",
    "ax.plot(fpr2,tpr2,label=\"Train Roc,Auc=\"+str(roc_auc_score(y_train,y_train_pred)))\n",
    "plt.title('ROC')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix using heat map for test data\n",
    "from sklearn.metrics import  confusion_matrix\n",
    "lr=LogisticRegression(penalty='l2',C=optimal_C_tfidf)\n",
    "lr.fit(Xtrain_tf_idf,y_train)\n",
    "pred=lr.predict(Xtest_tf_idf)\n",
    "\n",
    "import seaborn as sns\n",
    "con_mat=confusion_matrix(y_test,pred)\n",
    "class_label=['negative','positive']\n",
    "df=pd.DataFrame(con_mat,index=class_label,columns=class_label)\n",
    "sns.heatmap(df,annot=True,fmt='d')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance on TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = tf_idf_vect.get_feature_names()\n",
    "weight=lr.coef_\n",
    "#x = np.array([3, 1, 2])\n",
    "#np.argsort(x)\n",
    "#output >> array([1, 2, 0])\n",
    "pos_index=np.argsort(weight)[:,::-1]\n",
    "neg_index=np.argsort(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Top 10 important features of positive class from\n",
    "print('Top 10 important features of positive class ')\n",
    "for i in list(pos_index[0][:10]):\n",
    "    print(all_features[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 10 important features of negative class -=\n",
    "print(\"top 10 negative features:\")\n",
    "for i in list(neg_index[0][0:10]):\n",
    "    print(all_features[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "list_of_Xtr=[]\n",
    "for sent in X_train:\n",
    "    list_of_Xtr.append(sent.split())\n",
    "print(\"len of list_of_Xtr :\",len(list_of_Xtr))\n",
    "\n",
    "i=0\n",
    "list_of_Xtest=[]\n",
    "for sent in X_test:\n",
    "    list_of_Xtest.append(sent.split())\n",
    "print(\"len of list_of_Xtest :\",len(list_of_Xtest))\n",
    "\n",
    "i=0\n",
    "list_of_Xcv=[]\n",
    "for sent in X_cv:\n",
    "    list_of_Xcv.append(sent.split())\n",
    "print(\"len of list_of_Xcv :\",len(list_of_Xcv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Train the word2vec model using train data\n",
    "w2v_model=Word2Vec(list_of_Xtr,min_count=2,size=50,workers=4)\n",
    "\n",
    "#min_count :means if a word doesnt occur atleast 5 times don't create word2vec\n",
    "# vector_size :is the number of dimensions (N) of the N-dimensional space that gensim Word2Vec maps the words onto.\n",
    "#workers : the last of the major parameters (full list here) is for training parallelization, to speed up training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_words=list(w2v_model.wv.vocab)\n",
    "print(\"number of words that occured minimum 2 times \",len(w2v_words))\n",
    "print('='*50)\n",
    "print(\"sample words \", w2v_words[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w2v_model.wv['great'])\n",
    "print('='*50)\n",
    "print(len(w2v_model.wv['great']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w2v_model.wv.most_similar('great'))\n",
    "print('='*50)\n",
    "print(w2v_model.wv.most_similar('worst'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting text into vectors using Avg W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Avg W2Vec on train data \n",
    "#computing average word2vec for each reviews\n",
    "sent_vectors_train=[]\n",
    "for sent in tqdm(list_of_Xtr):\n",
    "    sent_vec=np.zeros(50)\n",
    "    count_words=0\n",
    "    for word in sent:\n",
    "        if word in w2v_words:\n",
    "            vec=w2v_model.wv[word]\n",
    "            sent_vec+=vec\n",
    "            count_words+=1\n",
    "    if count_words!=0:\n",
    "        sent_vec/=count_words\n",
    "    sent_vectors_train.append(sent_vec)\n",
    "print(len(sent_vectors_train))\n",
    "print(len(sent_vectors_train[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Avg W2Vec on test data \n",
    "#computing average word2vec for each reviews\n",
    "sent_vectors_test=[]\n",
    "for sent in tqdm(list_of_Xtest):\n",
    "    sent_vec=np.zeros(50)\n",
    "    count_words=0\n",
    "    for word in sent:\n",
    "        if word in w2v_words:\n",
    "            vec=w2v_model.wv[word]\n",
    "            sent_vec+=vec\n",
    "            count_words+=1\n",
    "    if count_words!=0:\n",
    "        sent_vec/=count_words\n",
    "    sent_vectors_test.append(sent_vec)\n",
    "print(len(sent_vectors_test))\n",
    "print(len(sent_vectors_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Avg W2Vec on cv data \n",
    "#computing average word2vec for each reviews\n",
    "sent_vectors_cv=[]\n",
    "for sent in tqdm(list_of_Xcv):\n",
    "    sent_vec=np.zeros(50)\n",
    "    count_words=0\n",
    "    for word in sent:\n",
    "        if word in w2v_words:\n",
    "            vec=w2v_model.wv[word]\n",
    "            sent_vec+=vec\n",
    "            count_words+=1\n",
    "    if count_words!=0:\n",
    "        sent_vec/=count_words\n",
    "    sent_vectors_cv.append(sent_vec)\n",
    "print(len(sent_vectors_cv))\n",
    "print(len(sent_vectors_cv[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_avg=sent_vectors_train\n",
    "x_test_avg=sent_vectors_test\n",
    "x_cv_avg=sent_vectors_cv\n",
    "\n",
    "scaler =StandardScaler(with_mean=False)\n",
    "scaler.fit(x_train_avg)\n",
    "\n",
    "x_train_avg=scaler.transform(x_train_avg)\n",
    "x_test_avg=scaler.transform(x_test_avg)\n",
    "x_cv_avg=scaler.transform(x_cv_avg)\n",
    "\n",
    "print(len(x_train_avg))\n",
    "print(len(x_train_avg[0]))\n",
    "\n",
    "print(len(x_test_avg))\n",
    "print(len(x_test_avg[0]))\n",
    "\n",
    "print(len(x_cv_avg))\n",
    "print(len(x_cv_avg[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding optimal_alpha by using 10-fold_cv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "alpha_C=[0.00001,0.0001,0.001,0.01,0.1,1,10,100,1000,10000,100000]\n",
    "cv_score_avg_w2v=[]\n",
    "#perform 10-foldcv\n",
    "for i in tqdm(alpha_C):\n",
    "    lr=LogisticRegression(penalty='l2',C=i)\n",
    "    score_avg_W2v=cross_val_score(lr,x_train_avg,y_train,cv=10)\n",
    "    cv_score_avg_w2v.append(score_avg_W2v.mean())\n",
    "    \n",
    "#Changing to misclassification error\n",
    "MSE=[1-x for x in cv_score_avg_w2v]\n",
    "\n",
    "#determine  best alpha\n",
    "optimal_C_avg_W2v=alpha_C[MSE.index(min(MSE))]\n",
    "print(\"Optimal c:\",optimal_C_avg_W2v)\n",
    "alpha_C=[math.log(x) for x in alpha_C]\n",
    "#Plot the misclassfication error vs alpha\n",
    "plt.plot(alpha_C,MSE,marker='*')\n",
    "plt.title(\"Misclassification Error vs alpha\")\n",
    "plt.xlabel('value of alpha')\n",
    "plt.ylabel('Misclassification Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roc \n",
    "lr =LogisticRegression(penalty='l2',C=optimal_C_avg_W2v)\n",
    "lr.fit(x_train_avg,y_train)\n",
    "\n",
    "y_test_pred=lr.predict_proba(x_test_avg)[:,1]\n",
    "fpr1,tpr1,thresholds1=metrics.roc_curve(y_test,y_test_pred)\n",
    "\n",
    "y_train_pred=lr.predict_proba(x_train_avg)[:,1]\n",
    "fpr2,tpr2,thresholds2=metrics.roc_curve(y_train,y_train_pred)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "ax.plot(fpr1, tpr1, label='Test ROC ,auc='+str(roc_auc_score(y_test,y_test_pred)))\n",
    "ax.plot(fpr2, tpr2, label='Train ROC ,auc='+str(roc_auc_score(y_train,y_train_pred)))\n",
    "plt.title('ROC')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix using heatmap for test data\n",
    "from sklearn.metrics import confusion_matrix\n",
    "lr=LogisticRegression(penalty='l2',C=optimal_C_avg_W2v)\n",
    "lr.fit(x_train_avg,y_train)\n",
    "y_predict=lr.predict(x_test_avg)\n",
    "\n",
    "con_matrix=confusion_matrix(y_test,y_predict)\n",
    "class_label=[\"negative\", \"positive\"]\n",
    "df=pd.DataFrame(con_matrix,index=class_label,columns=class_label)\n",
    "sns.heatmap(df,annot=True,fmt='d')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Logistic Regression with L2 regularization on TFIDF W2V,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_tr ,X_test,y_tr,y_test = train_test_split(X,y,test_size=.30,random_state=0)\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X_tr, y_tr, test_size=.30, random_state=0)\n",
    "print('Shape of X_train is :',X_train.shape)\n",
    "print('Shape of y_train is :',y_train.shape)\n",
    "print(\"****\"*6)\n",
    "print('Shape of X_Cv is :',X_cv.shape)\n",
    "print('Shape of y_cv is :',y_cv.shape)\n",
    "print(\"****\"*6)\n",
    "print('Shape of X_test is :',X_test.shape)\n",
    "print('Shape of y_test is :',y_test.shape)\n",
    "print(\"****\"*6)\n",
    "print('Shape of X_tr is :',X_tr.shape)\n",
    "print('Shape of y_tr is :',y_tr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "list_of_Xtr=[]\n",
    "for sent in X_train:\n",
    "    list_of_Xtr.append(sent.split())\n",
    "print(\"len of list_of_Xtr :\",len(list_of_Xtr))\n",
    "\n",
    "i=0\n",
    "list_of_Xtest=[]\n",
    "for sent in X_test:\n",
    "    list_of_Xtest.append(sent.split())\n",
    "print(\"len of list_of_Xtest :\",len(list_of_Xtest))\n",
    "\n",
    "i=0\n",
    "list_of_Xcv=[]\n",
    "for sent in X_cv:\n",
    "    list_of_Xcv.append(sent.split())\n",
    "print(\"len of list_of_Xcv :\",len(list_of_Xcv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Train the word2vec model\n",
    "w2v_model=gensim.models.Word2Vec(list_of_Xtr,min_count=5,size=50,workers=4)\n",
    "#min_count :means if a word doesnt occur atleast 5 times don't create word2vec\n",
    "# vector_size :is the number of dimensions (N) of the N-dimensional space that gensim Word2Vec maps the words onto.\n",
    "#workers : the last of the major parameters (full list here) is for training parallelization, to speed up training:\n",
    "w2vwords=list(w2v_model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_model=TfidfVectorizer()\n",
    "tf_idf_model.fit_transform(X_train)\n",
    "#creating a dictinory with a word  as a key and the idf as value\n",
    "dictionary=dict(zip(tf_idf_model.get_feature_names(),list(tf_idf_model.idf_)))\n",
    "\n",
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_feat=tf_idf_model.get_feature_names()\n",
    "\n",
    "tf_idf_sent_vect_train=[]\n",
    "row=0\n",
    "for sent in tqdm(list_of_Xtr):\n",
    "    sent_vec=np.zeros(50)\n",
    "    weight_sum=0\n",
    "    for word in sent:\n",
    "        if word in w2vwords and word in tfidf_feat:\n",
    "            vec=w2v_model.wv[word]\n",
    "            #tf_idf=final_tf_idf[row,tfidf_feat.index(word)]\n",
    "            # to reduce the computation we are computing \n",
    "            #dictionary[word]:idf value of the word in whole corpus \n",
    "            #dictionary['aberfoyl get'] output:>>11.263449588064944\n",
    "            #sent.count(word):tfvalues of word in this reviews\n",
    "            tf_idf=dictionary[word]*(sent.count(word)/len(sent))\n",
    "            #sent.count(word) gives us the count of word in sentence and\n",
    "            #then we're dividing this term by total number of words in sent that gives us tf value.\n",
    "            sent_vec+=(vec*tf_idf)\n",
    "            weight_sum+=tf_idf\n",
    "    if weight_sum!=0:\n",
    "        sent_vec/=weight_sum\n",
    "    tf_idf_sent_vect_train.append(sent_vec)\n",
    "    row+=1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tf_idf_sent_vect_train))\n",
    "print(len(tf_idf_sent_vect_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_feat=tf_idf_model.get_feature_names()\n",
    "\n",
    "tf_idf_sent_vect_test=[]\n",
    "row=0\n",
    "for sent in tqdm(list_of_Xtest):\n",
    "    sent_vec=np.zeros(50)\n",
    "    weight_sum=0\n",
    "    for word in sent:\n",
    "        if word in w2vwords and word in tfidf_feat:\n",
    "            vec=w2v_model.wv[word]\n",
    "            #tf_idf=final_tf_idf[row,tfidf_feat.index(word)]\n",
    "            # to reduce the computation we are computing \n",
    "            #dictionary[word]:idf value of the word in whole corpus \n",
    "            #dictionary['aberfoyl get'] output:>>11.263449588064944\n",
    "            #sent.count(word):tfvalues of word in this reviews\n",
    "            tf_idf=dictionary[word]*(sent.count(word)/len(sent))\n",
    "            #sent.count(word) gives us the count of word in sentence and\n",
    "            #then we're dividing this term by total number of words in sent that gives us tf value.\n",
    "            sent_vec+=(vec*tf_idf)\n",
    "            weight_sum+=tf_idf\n",
    "    if weight_sum!=0:\n",
    "        sent_vec/=weight_sum\n",
    "    tf_idf_sent_vect_test.append(sent_vec)\n",
    "    row+=1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tf_idf_sent_vect_test))\n",
    "print(len(tf_idf_sent_vect_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_sent_vect_cv=[]\n",
    "row=0\n",
    "for sent in tqdm(list_of_Xcv):\n",
    "    sent_vec=np.zeros(50)\n",
    "    weight_sum=0\n",
    "    for word in sent:\n",
    "        if word in w2vwords and word in tfidf_feat:\n",
    "            vec=w2v_model.wv[word]\n",
    "            #tf_idf=final_tf_idf[row,tfidf_feat.index(word)]\n",
    "            # to reduce the computation we are computing \n",
    "            #dictionary[word]:idf value of the word in whole corpus \n",
    "            #dictionary['aberfoyl get'] output:>>11.263449588064944\n",
    "            #sent.count(word):tfvalues of word in this reviews\n",
    "            tf_idf=dictionary[word]*(sent.count(word)/len(sent))\n",
    "            #sent.count(word) gives us the count of word in sentence and\n",
    "            #then we're dividing this term by total number of words in sent that gives us tf value.\n",
    "            sent_vec+=(vec*tf_idf)\n",
    "            weight_sum+=tf_idf\n",
    "    if weight_sum!=0:\n",
    "        sent_vec/=weight_sum\n",
    "    tf_idf_sent_vect_cv.append(sent_vec)\n",
    "    row+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tf_idf_sent_vect_cv))\n",
    "print(len(tf_idf_sent_vect_cv[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tf_df_w2vec=tf_idf_sent_vect_train\n",
    "X_test_tf_df_w2vec=tf_idf_sent_vect_test\n",
    "X_cv_tf_df_w2vec=tf_idf_sent_vect_cv\n",
    "\n",
    "C_ = [10**-3, 10**-2, 10**0, 10**2,10**3,10**4]#C=1/lambda\n",
    "auc_train=[]\n",
    "auc_cv=[]\n",
    "for c in C_:\n",
    "    lr=LogisticRegression(penalty='l2',C=c)\n",
    "    lr.fit(X_train_tf_df_w2vec,y_train)\n",
    "    \n",
    "    y_pred_cv=lr.predict_proba(X_cv_tf_df_w2vec)[:,1]\n",
    "    auc_cv.append(roc_auc_score(y_cv,y_pred_cv))\n",
    "    \n",
    "    y_pred_train=lr.predict_proba(X_train_tf_df_w2vec)[:,1]\n",
    "    auc_train.append(roc_auc_score(y_train,y_pred_train))\n",
    "    \n",
    "    auc_score_cv=roc_auc_score(y_cv,y_pred_cv)\n",
    "    print(c,\"-------->\",auc_score_cv )\n",
    "\n",
    "optimal_c_tfidf_w2v=C_[auc_cv.index(max(auc_cv))]\n",
    "c=[math.log(x) for x in C_]\n",
    "print(\"Optimal c:\",optimal_c_tfidf_w2v)\n",
    "fig=plt.figure()\n",
    "plt.plot(c,auc_train,color='green',label='Auc_Train')\n",
    "plt.plot(c,auc_cv,color='red',label='Auc_Test')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('log(alpha')\n",
    "plt.ylabel('AUC')\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Roc \n",
    "lr=LogisticRegression(penalty='l2',C=optimal_c_tfidf_w2v)\n",
    "lr.fit(X_train_tf_df_w2vec,y_train)\n",
    "    \n",
    "y_pred_test=lr.predict_proba(X_test_tf_df_w2vec)[:,1]\n",
    "fpr1,tpr1,thresholds1=metrics.roc_curve(y_test,y_pred_test)\n",
    "\n",
    "y_pred_train=lr.predict_proba(X_train_tf_df_w2vec)[:,1]\n",
    "fpr2,tpr2,thresholds2=metrics.roc_curve(y_train,y_pred_train)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "ax.plot(fpr1, tpr1, label='Test ROC ,auc='+str(roc_auc_score(y_test,y_pred_test)))\n",
    "ax.plot(fpr2, tpr2, label='Train ROC ,auc='+str(roc_auc_score(y_train,y_pred_train)))\n",
    "plt.title('ROC')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix using Heat map for test data\n",
    "#Confusion matrix using heatmap for test data\n",
    "from sklearn.metrics import confusion_matrix\n",
    "lr=LogisticRegression(penalty='l2',C=optimal_c_tfidf_w2v)\n",
    "lr.fit(X_train_tf_df_w2vec,y_train)\n",
    "y_predic=lr.predict(X_test_tf_df_w2vec)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns    \n",
    "conf_matrix=confusion_matrix(y_test,y_predic)\n",
    "class_label=['negative','postive']\n",
    "df=pd.DataFrame(conf_matrix,index=class_label,columns=class_label)\n",
    "sns.heatmap(df,annot=True,fmt='d')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please compare all your models using Prettytable library\n",
    "from prettytable import PrettyTable    \n",
    "x = PrettyTable()\n",
    "x.field_names = [\"Vectorizer\",\"Regularization\", \"Feature engineering\", \"Hyperameter\", \"AUC\"]\n",
    "x.add_row([\"BOW\",\"l2\",\"Not featured\",0.0001,0.9398])\n",
    "x.add_row([\"TFIDF\",\"l2\",\"Not featured\",10000,0.9688])\n",
    "x.add_row([\"Avg W2v\",\"l2\",\"Not featured\",10,0.9184])\n",
    "x.add_row([\"TFIDF Avg W2v\",\"l2\",\"Not featured\",1,0.8971])\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First we split whole preprocessed review data to train ,cross validation and test \n",
    "2. Then we applied logistic regression on Bag of Words vectoriser for l2 regularisation then measured best hyperparameter and plotted ROC curve and heatmap on test data \n",
    "3. After that we wrote down top 10 postive and negative features \n",
    "4. Second we applied logistic regression on TFIDF vectoriser for l2 regularisation ,then measured best lambda value and performed same process like Bag of words. \n",
    "5. For Avg Word 2 vec we perform w2v on train data and used models and words of train data to cv and test data \n",
    "6. Then after we applied logistic regression on Avg Word2vec for l2 regularisation \n",
    "7. Just like Avg word2vec we applied logistic regression on TFIDF avgW2vec \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
